{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2017 August 06, Tang Li Qun\n",
    "# Project Standy : Uzu Uzu Index modelling\n",
    "# 4 layer regression multi layer perceptron (MLP) for modelling the uzu uzu index (UUI) used in project Standy\n",
    "# feel free to play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var command = \"nb_name = '\" + IPython.notebook.notebook_path + \"'; nb_name = nb_name.split('/')[-1]\";\n",
       "IPython.notebook.kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var command = \"nb_name = '\" + IPython.notebook.notebook_path + \"'; nb_name = nb_name.split('/')[-1]\";\n",
    "IPython.notebook.kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liqun\\Desktop\\hackathon\\modules\\uzuuzuindex_nn\\uzuuzuindex_nn.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nb_full_path = os.path.join(os.getcwd(), nb_name)\n",
    "print(nb_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrix processing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import sys\n",
    "\n",
    "# neural network pieces\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# neural network metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# misc\n",
    "import os, pickle\n",
    "\n",
    "# reference: doesn't work in jupyter notebook\n",
    "#print(__file__)\n",
    "#directory = os.path.dirname(__file__)\n",
    "directory = os.path.dirname(nb_full_path)\n",
    "\n",
    "dataset = pd.read_csv(directory + r'/ml_dataset/tokyo_dataset_uui2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "this is your input :\n",
      "   weekday  cloud_cover  dew_point  rain_mm  rh  snow_cover  sunlight_hrs  \\\n",
      "0        0            5       20.8      0.2  56           0          1.96   \n",
      "\n",
      "   temp_celsius  visibility  weather_rating  forest area coverage  LowPressure  \n",
      "0          30.6          10               2              0.087407        0.655  \n",
      "Loaded model from disk\n",
      "prediction inputs required:\n",
      "day of the week = 'ddd'\n",
      "        cloud cover\n",
      "        dew point\n",
      "        rain\n",
      "        relative humidity\n",
      "        snow cover\n",
      "        sunlight hours\n",
      "        temperature\n",
      "        weather visibility\n",
      "        weather rating\n",
      "        forest area\n",
      "        low pressure index\n",
      "predictions: [[ 0.53388011]]\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "def hackathon_preprocess(dataset):\n",
    "    \"\"\"\n",
    "    extremely shitty, only takes one form of input csv\n",
    "    :param dataset: dirty dataset\n",
    "    :return: cleaned dataset\n",
    "    \"\"\"\n",
    "    dataset = dataset.ix[:, 2:]  # takes out first column (previous index number) and second column (datse)\n",
    "    dataset_a = dataset.ix[:, 0]  # +below: takes out atmospheric pressure\n",
    "    dataset_b = dataset.ix[:, 2:]\n",
    "    dataset = pd.concat([dataset_a, dataset_b], axis=1)\n",
    "\n",
    "    dataset.replace(to_replace=['10-', '0+'], value=0, inplace=True)\n",
    "    dataset.replace(to_replace=['Mon', 'Tue', 'Wed', 'Thu', 'Fri'], value=0, inplace=True)\n",
    "    dataset.replace(to_replace=['Sat', 'Sun'], value=1, inplace=True)\n",
    "    dataset = dataset.fillna(value=0)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def uzu_uzu_index_predictor(train_dataset=None, new_input=False, load_saved_model=True, save_model=True, plot_results=False):\n",
    "    \"\"\"\n",
    "    defaults to loading pretrained model. if load_saved_model is False, it will retrain on the dataset in the accompanying folder.\n",
    "    either pretrained or new NN will be used to predict the set of new inputs and return an uzu uzu index\n",
    "\n",
    "    :param train_dataset: FUTURE - feed dataset from outside\n",
    "    :param new_input: input list for prediction (refer to uui_calculator for details)\n",
    "    :param load_saved_model: default = True. if False, will retrain model\n",
    "    :param save_model: default = True. will only save model if load_saved_model is False and save_model is True\n",
    "    :param plot_results: use matplotlib to plot the results as a graph (for checking accuracy during training)\n",
    "    :return: uzu uzu index\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if new_input is not False:\n",
    "        #print('receiving input: ', new_input)\n",
    "        col_labels = ['ind',\n",
    "                      'datetime',\n",
    "                      'weekday',\n",
    "                      'atm_pressure',\n",
    "                      'cloud_cover',\n",
    "                      'dew_point',\n",
    "                      'rain_mm',\n",
    "                      'rh',\n",
    "                      'snow_cover',\n",
    "                      'sunlight_hrs',\n",
    "                      'temp_celsius',\n",
    "                      'visibility',\n",
    "                      'weather_rating',\n",
    "                      'forest area coverage',\n",
    "                      'LowPressure']\n",
    "        new_input.insert(0, 0)\n",
    "        new_input.insert(0, 0)\n",
    "        new_input.insert(3, 0)\n",
    "        new_input = pd.DataFrame(data=[new_input], columns=col_labels)\n",
    "        new_input = hackathon_preprocess(new_input)\n",
    "        # show processed inputs\n",
    "        print('\\nthis is your input :')\n",
    "        print(new_input)\n",
    "\n",
    "\n",
    "    # if True, load pretrained model\n",
    "    if load_saved_model:\n",
    "        # load json and create model\n",
    "        json_file = open(directory + r'/trained_nn/model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        model.load_weights(directory + r\"/trained_nn/model.h5\")\n",
    "        scaler_x = pickle.load(open(directory + r'/trained_nn/model_datascaler.p', 'rb'))\n",
    "        print(\"Loaded model from disk\")\n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        # prediction inputs required\n",
    "        print('prediction inputs required:')\n",
    "        #temp = '278,09/08/2016 15:00,Tue,998,6,19.6,0,38,,2.47,36.2,20,2,0.087407407,0.900249511,5.810333102'.split(',')\n",
    "        print(\"\"\"day of the week = 'ddd'\n",
    "        cloud cover\n",
    "        dew point\n",
    "        rain\n",
    "        relative humidity\n",
    "        snow cover\n",
    "        sunlight hours\n",
    "        temperature\n",
    "        weather visibility\n",
    "        weather rating\n",
    "        forest area\n",
    "        low pressure index\"\"\")\n",
    "\n",
    "\n",
    "    # else do the full training\n",
    "    else:\n",
    "\n",
    "        dataset = hackathon_preprocess(dataset)\n",
    "\n",
    "        row_len, col_len = dataset.shape\n",
    "        feature_len = col_len - 1\n",
    "        print('number of data points:', row_len)\n",
    "        print('number of features considered', feature_len)\n",
    "\n",
    "\n",
    "        # fix random seed for reproducibility\n",
    "        seed = 7\n",
    "        numpy.random.seed(seed)\n",
    "\n",
    "\n",
    "        # get train and test ratio\n",
    "        # split dataset into sets\n",
    "        # might need padding if the length is different\n",
    "        ratio = 0.8  # train/test ratio\n",
    "        length = len(dataset.index)\n",
    "        split_i = ratio * length\n",
    "\n",
    "        x_train = dataset.ix[:split_i, :-1]\n",
    "        y_train = dataset.ix[:split_i, -1]\n",
    "\n",
    "        x_test = dataset.ix[split_i+1:, :-1]\n",
    "        y_test = dataset.ix[split_i+1:, -1]\n",
    "\n",
    "\n",
    "        # turn dataframe into numpy arrays\n",
    "        # required for sklearn stuff\n",
    "        x_train = x_train.as_matrix()\n",
    "        y_train = y_train.as_matrix()\n",
    "\n",
    "        x_test = x_test.as_matrix()\n",
    "        y_test = y_test.as_matrix()\n",
    "\n",
    "        # normalize data\n",
    "        # fit scaler\n",
    "        scaler_x = MinMaxScaler().fit(x_train)\n",
    "        scaler_y = MinMaxScaler().fit(y_train)\n",
    "        x_train = scaler_x.transform(x_train)\n",
    "        x_test = scaler_x.transform(x_test)\n",
    "        y_train = scaler_y.transform(y_train)\n",
    "        y_test = scaler_y.transform(y_test)\n",
    "\n",
    "        # save scaler for transforming new datasets\n",
    "        pickle.dump(scaler_x, open(directory + r'/trained_nn/model_datascaler.p', 'wb'))\n",
    "\n",
    "        # neural networks to use\n",
    "\n",
    "        def regression_nn():\n",
    "            # define regression model\n",
    "            # create model\n",
    "            model = Sequential()\n",
    "\n",
    "            # input layer + 20 neuron hidden layer\n",
    "            model.add(Dense(20, input_dim=feature_len, kernel_initializer='normal'))\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "            # 30 neuron hidden layer\n",
    "            model.add(Dense(30, kernel_initializer='normal'))\n",
    "            #model.add(BatchNormalization())\n",
    "            model.add(Activation('relu'))\n",
    "\n",
    "            # final nonlinear bit\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation('tanh'))\n",
    "            # compile\n",
    "            # adam optimizer is super fast compared to sgd\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            return model\n",
    "\n",
    "        def lstm_embedded_nn():\n",
    "            \"\"\"\n",
    "            NOPE.\n",
    "            :return:\n",
    "            \"\"\"\n",
    "\n",
    "            # initialize\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Embedding(feature_len, output_dim=64))\n",
    "            model.add(LSTM(32))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "            model.compile(loss='mse',\n",
    "                          optimizer='adam')\n",
    "            return model\n",
    "\n",
    "\n",
    "        model = regression_nn()\n",
    "        # regression model params\n",
    "        model.fit(x_train, y_train, epochs=500, batch_size=24, verbose=1)\n",
    "        # simple lstm model params\n",
    "        #model.fit(x_train, y_train, epochs=300, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "        # make prediction\n",
    "        predictions = model.predict(x_test)\n",
    "\n",
    "        # score prediction for regression\n",
    "        # sklearn accuracy_score only for classification\n",
    "        test_mse = mean_squared_error(y_test, predictions)\n",
    "        print('test mse : ', test_mse)\n",
    "\n",
    "        # score prediction for lstm\n",
    "\n",
    "\n",
    "        result_pred = scaler_y.inverse_transform(predictions)\n",
    "        result_truth = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "        output_pred = [x[0] for x in predictions.tolist()]\n",
    "        output_truth = y_test.tolist()\n",
    "\n",
    "        avg_accuracy = []\n",
    "        for i, _ in enumerate(output_pred):\n",
    "            guess = output_pred[i]\n",
    "            truth = output_truth[i]\n",
    "            #print('%.2f' % guess, ' > ', '%.2f' % truth)\n",
    "            num_distance = abs(guess - truth)\n",
    "            accuracy = (1 - abs(num_distance - 0)) * 100\n",
    "            avg_accuracy.append(accuracy)\n",
    "\n",
    "        print('average accuracy %.2f%%' %(sum(avg_accuracy) / float(len(avg_accuracy))))\n",
    "\n",
    "        if plot_results:\n",
    "            # show in a graph how accurate\n",
    "            plt.plot(result_pred, color=\"blue\")\n",
    "            plt.plot(result_truth, color=\"green\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    # save trained model\n",
    "    # serialize model to JSON\n",
    "    if not load_saved_model:\n",
    "        if save_model:\n",
    "            model_json = model.to_json()\n",
    "            with open(directory + \"/trained_nn/model.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(directory + \"/trained_nn/model.h5\")\n",
    "            print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "    if new_input is not False:\n",
    "        # make new prediction\n",
    "        z_test = scaler_x.transform(new_input)\n",
    "        predictions = model.predict(z_test)\n",
    "    else:\n",
    "        predictions = [[0]]\n",
    "\n",
    "    \n",
    "    print('predictions:', predictions)\n",
    "    predictions = [x[0] for x in predictions.tolist()]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "test_input = ['Wed',5,20.8,0.2,56,0,1.96,30.6,10,2,0.087407407,0.655]\n",
    "uui = uzu_uzu_index_predictor(dataset, new_input=test_input, load_saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
